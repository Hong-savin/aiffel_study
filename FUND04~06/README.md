# FUND 04~06

---

## 4. 사이킷런으로 구현해보능 머신러닝
- **머신러닝 알고리즘**
 - **지도학습**: 레이블이 있는 데이터를 사용해 모델을 학습시키는 방법으로, 입력과 출력 간의 매핑을 학습. (예: 분류, 회귀, 예측)
 - **비지도학습**: 레이블이 없는 데이터를 사용해 패턴이나 구조를 찾는 학습 방법으로, 데이터의 잠재적인 구조를 파악. (예: 군집화, 차원 축소.)
 - **준지도학습**: 적은 양의 레이블된 데이터와 많은 양의 레이블되지 않은 데이터를 함께 사용해 학습하는 방법으로, 라벨이 부족한 상황에서 성능을 개선할 수 있음. (예: 반정형 데이터 학습.)
 - **강화학습**: 에이전트가 환경과 상호작용하며 보상을 최대화하는 행동을 학습하는 방법으로, 보상 기반의 학습 과정을 거침. (예: 게임 AI, 로봇 제어.)
 - **선형회귀**: 연속적인 값을 예측하고자 할 때 사용. 하나 이상의 독립변수와 종속변수 사이의 관계를 모델링.
 - **로지스틱 회귀**: 분류 문제에서 사용. 선형회귀에 로지스틱 함수를 붙여 0과 1 사이의 확률을 구함. 이 확률은 특정 클래스에 속할 확률이 됨. Softmax 값을 사용하여 다중 분류에 적용.
 - **선형 / 커널 SVM**: 선형 SVM은 데이터를 선형적으로 분리하는 초평면을 찾고, 커널 SVM은 비선형적으로 분리 가능한 데이터를 고차원으로 변환해 분류하는 방법. (예: 얼굴 인식, 문서 분류.)
 - **의사결정 나무**: 데이터를 분할해 결정 규칙을 생성하고, 이를 기반으로 예측을 수행하는 트리 구조의 학습 모델. (예: 고객 분류, 신용 평가.)
 - **앙상블 트리**: 여러 개의 의사결정 나무를 결합해 더 강력한 예측 모델을 만드는 방법으로, 일반적으로 배깅, 부스팅 기법이 사용됨. (예: 랜덤 포레스트, 그라디언트 부스팅 머신.)
 - **Estimator 객체**: 데이터셋을 기반으로 머신러닝 모델의 파라미터를 추정하는 객체. Estimator의 fit() 메서드를 통해 훈련이 이루어지고, predict() 메서드를 통해 예측이 이루어짐. -> 비지도학습의 경우 fit() 메서드 인자로 target vector가 들어가지 않음.

---

## 5. Regularization
- **Overfiting**: train data set은 잘 맞추지만 validation, test set에서는 잘 못 맞춤
- **Regularization(정칙화)**: 오버피팅을 해결하기 위한 방법 중 하나
- **Normalization(정규화)**: 데이터의 형태를 좀 더 의미있게, 혹은 트레이닝에 적합한 전처리 과정 -> 트레이닝을 할 때에 서로 범위가 다른 데이터들을 같은 범위로 바꿔주는 전처리 과정
- **Norm**: 벡터나 행렬, 함수 등의 거리를 나타내는 것
- **L1 Regularization, L2 Regularization**: Lasso는 가중치들이 0이 되지만, Ridge의 가중치들은 0에 가까워질 뿐 0이 되진 않음. -> L1 Regularization은 가중치가 적은 벡터에 해당하는 계수를 0으로 보내면서 차원 축소와 비슷한 역할, L2 Regularization은 계수를 0으로 보내진 않지만 제곱 텀이 있기 때문에 L1 Regularization보다 수렴 속도가 빠름.
 - 공통점: 과적합을 방지한다.
 - 차이점: L1은 불필요한 특성을 제거하여 모델을 단순화하고, L2는 모든 특성을 유지하며 모델의 복잡성을 줄임.
- **Dropout**: 확률적으로 랜덤하게 몇 가지의 뉴런만 선택하여 정보를 전달하는 과정. 몇 가지의 값들을 모든 뉴런에 전달하는 것이 아닌, 확률적으로 버리면서 전달하는 기법.

---

## 6. 비지도학습
- **K-means**:
 1. 클러스터의 개수(K) 값 설정
 2. K-means 클러스팅을 무작위로 초기 중심점 선택(무작위 분할)
 3. 각 데이터 포인트를 가장 가까운 중심점에 핳당
 4. 할당된 클러스터의 평균으로 중심점 업데이트
 5. K-means 클러스터링 알고리즘을 수행
- **DBSCAN**:
 1. 임의의 점 p를 설정하고, p를 포함하여 주어진 클러스터의 반경 안에 포함되어 있는 점들의 개수 파악.
 2. 만일 해당 원에 minPts개 이상의 점이 포함되어 있으면, 해당 점 p를 core point로 간주하고 원에 포함된 점들을 하나의 클러스터로 묶음.
 3. 해당 원에 minPts개 미만의 점이 포함되어 있으면, 일단 pass.
 4. 모든 점에 대하여 1 ~ 3번의 과정을 반복, 만일 새로운 점 p'가 core point가 되고 이 점이 기존의 클러스터에 속한다면, 두 개의 클러스터는 연결되어 있다 파악하고 하나의 클러스터로 묶음.
 5. 어떤 점을 중심으로 하더라도 클러스터에 속하지 못하는 점이 있으면 이를 noise point로 간주함. 또한 특정 군집에는 속하지만 core point가 아닌 점들을 border point라고 칭힘.
 - 데이터의 수가 적을 때는 K-means 알고리즘 수행 시간이 DBSCAN에 비해 길었으나, 군집화할 데이터가 많아질 수록 DBSCAN의 알고리즘 수행 시간이 급격하게 늘어남.
- **차원 축소(PCA - 주성분분석)**: X-Y-Z 좌표축 상에 존재하는 데이터를 X-Y, Y-Z 좌표축에 projection(사영) 했다는 것은 각각 Z, X 좌표축을 무시했다는 뜻.
- **차원 축소(T-SNE)**: PCA를 통해 차원축소를 하면서 발생하는 정보 손실의 과정 중에는 두 점 사이의 거리라는 중요한 정보가 함께 손실되는 측면이 있음. 만약 두 점의 거리가 PCA의 PC축을 따라 발생한 거리라면 유지되겠지만, 그렇지 않다면 PCA 과정을 통해 두 점 사리의 거리가 소거되고, 실제론 먼 거리의 점들이 아주 가까운 점들로 투영될 가능성이 있음.
